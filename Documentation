Prerequisites
To complete this tutorial, you will need the following:

An Ubuntu 22.04 server with 4GB RAM and 2 CPUs(t3 medium) set up with a non-root sudo user. You can achieve this by following the Initial Server Setup with Ubuntu 22.04.For this tutorial, we will work with the minimum amount of CPU and RAM required to run Elasticsearch. Note that the amount of CPU, RAM, and storage that your Elasticsearch server will require depends on the volume of logs that you expect.

This Lab is using t3 medium EC2 instance and needs to have following configured:

Install AWS CLI – Command line tools for working with AWS services, including Amazon EKS.
Follow these steps from the command line to install the AWS CLI on Linux.
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" 
sudo apt install unzip
sudo unzip awscliv2.zip  
sudo ./aws/install
aws --version

Install eksctl – A command line tool for working with EKS clusters that automates many individual tasks.
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
Move the extracted binary to /usr/local/bin. 
sudo mv /tmp/eksctl /usr/local/bin
eksctl version

Install kubectl  – A command line tool for working with Kubernetes clusters. 
sudo curl --silent --location -o /usr/local/bin/kubectl   https://s3.us-west-2.amazonaws.com/amazon-eks/1.22.6/2022-03-09/bin/linux/amd64/kubectl
sudo chmod +x /usr/local/bin/kubectl 
Verify if kubectl got installed
kubectl version --short --client

Create IAM Role with Administrator Access

You need to create an IAM role with AdministratorAccess policy.
Go to AWS console, IAM, click on Roles. create a role

Select AWS services, Click EC2, Click on Next permissions.
 Now search for AdministratorAccess policy and click

Skip on create tag.
Now give a role name and create it.

Assign the role to EC2 instance
Go to AWS console, click on EC2, select EC2 instance, Choose Security.
Click on Modify IAM Role

Choose the role you have created from the dropdown.
Select the role and click on Apply.

in git bash(linux environment) enter
aws configure

Create EKS Cluster with two worker nodes using eksctl
eksctl create cluster --name demo-eks --region us-east-2 --nodegroup-name my-nodes --node-type t3.medium --managed --nodes 2 

git clone git@github.com:idogbo1759/Kubernetes_Elk_Logging.git
cd monitoring/kubernetes-elk
kubectl apply -f elasticsearch-ss.yaml
kubectly get svc -n kube-system

kubectl get node -o wide
copy the external ip address and add 31335: example : http://3.14.66.232:31335/
go to ec2 cluster on aws, click on the EC2 instance that matches same ip address you copied, 
click on security and edit inbound rules and open two ports (31335 and 31336) using custom TCP and custom source, click save

check the url in your browser: : http://3.14.66.232:31335/
it should display
{
  "name" : "elasticsearch-logging-0",
  "cluster_name" : "kubernetes-logging",
  "cluster_uuid" : "UZ6JTh68RbCzOfAFAfUSMw",
  "version" : {
    "number" : "6.2.4",
    "build_hash" : "ccec39f",
    "build_date" : "2018-04-12T20:37:28.497551Z",
    "build_snapshot" : false,
    "lucene_version" : "7.2.1",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}

go back on GITbash and execute the codes below:
List all Application
Ls -al

Install LogStash using this command:
kubectl apply -f logstash-deployment.yaml

Install Filebeat using this command:
kubectl apply -f filebeat-ds.yaml

Install Metricbeat using this command:
kubectl apply -f metricbeat-ds.yaml

Install Kibana using this command:
kubectl apply -f kibana-deployment.yaml

Get External Node IP
kubectl get node -o wide
copy the external ip address and add 31335: example : http://3.14.66.232:31336/ 
paste on your web browser and your kibana will be up

Get Service
kubectl get svc -n kube-system

Get Pods 
kubectl get pods -n kube-system

Get Nodes
kubectl get nodes -n kube-system

Get Logs
kubectl logs <kibana-pod-name> -n logging

Describe Deployment
kubectl describe deployment my-kibana -n logging

Delete pods:
kubectl delete pods --all -n kube-system 

Delete cluster:
eksctl delete cluster --name demo-eks --region us-east-2  


Conclusion
In this tutorial, you’ve learned how to install and configure the Elastic Stack to collect and analyze system logs. Remember that you can send just about any type of log or indexed data to Logstash using Beats, but the data becomes even more useful if it is parsed and structured with a Logstash filter, as this transforms the data into a consistent format that can be read easily by Elasticsearch.

visit: https://youtu.be/kEJgC_gJsZM?feature=shared
